{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aktivierungsfunktion\n",
    "\n",
    "##### sigmoid vs. softmax\n",
    "- sigmoid: Zahlen einer Range werden auf einen Wert zwischen 0 und 1 verteilt (aber niemals genau 0 oder 1)\n",
    "- softmax: Zahlen einer Range ergeben genau 1 (sinnvoll, wenn man feste Kategorien wie die Kleidungsstücke hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/activation_functions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear: Keine Aktivierungsfunktion -> Wenn Neuron 12 vorhersagt, kommt auch eine 12 heraus\n",
    "- **Sigmoid**: Zahlen werden auf Skala zwischen 0 und 1 angenähert\n",
    "- Tanh: Ähnlich wie Sigmoid, aber die Skala ist zwischen -1 und 1\n",
    "- **ReLu** (rectified linear unit): Gibt für alle negativen Werte eine 0 aus. Damit kann man ungewünschte Werte \"unter den Teppich kehren\", indem man hier die Aktivierung reduziert.\n",
    "- LReLu: bei größeren Datenmengen besser als ReLu\n",
    "\n",
    "Welche Aktivierungsfunktion (**mit diesen ausprobieren**) besser ist, kann man für seine Problemstellung teilweise nachschauen, wenn andere Leute schon ähnliche Probleme hatten oder man muss ausprobieren. Es gibt keine wirkliche Erklärung, warum eine Funktion besser funktioniert als eine andere.\n",
    "Man kann nur ausprobieren und schauen, wo man die höchste Genauigkeit erreicht."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
