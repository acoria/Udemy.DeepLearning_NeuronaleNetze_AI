{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genauigkeit der Aussage berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel von vorher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vtewe\\python_projects\\udemy_ai\\.conda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# FashionMNIST: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def open_images(filename):\n",
    "    with gzip.open(filename, \"rb\") as file:\n",
    "        data = file.read()\n",
    "        return np.frombuffer(data, dtype=np.uint8, offset=16)\\\n",
    "            .reshape(-1, 28, 28)\\\n",
    "            .astype(np.float32)\n",
    "\n",
    "\n",
    "def open_labels(filename):\n",
    "    with gzip.open(filename, \"rb\") as file:\n",
    "        data = file.read()\n",
    "        return np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "    \n",
    "X_train = open_images(\"data/fashion/train-images-idx3-ubyte.gz\")\n",
    "y_train_categories = open_labels(\"data/fashion/train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "# Sequential: bedeutet, dass man Ebene für Ebene ein Modell definieren kann\n",
    "from keras.models import Sequential\n",
    "# Dense: Standard-Layer. Bedeutet, dass alle möglichen Pfeile zwischen allen Neuronen erstellt werden (alles mit allem verknüpfen)\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Hidden layer \n",
    "#  100 Neuronen\n",
    "#  input_shape: Eingabeformat der Daten (bsw. 784 Zahlen für das 28*28px Image)\n",
    "#  (784,): Tuple - Python braucht für ein Tuple mit nur einem Wert ein ,\n",
    "model.add(Dense(100, activation=\"sigmoid\", input_shape=(784,)))\n",
    "\n",
    "# Ausgabeschicht\n",
    "#  Hier wird nur ein Neuron gebraucht\n",
    "model.add(Dense(1, activation=\"sigmoid\" ))\n",
    "\n",
    "# generiert effizienteren Code. Nötig, falls es später auf einer GPU ausgeführt wird\n",
    "# optimizer: Welches Verfahren soll verwendet werden, um die Gewichte zu verstellen (stochastic gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beim Kompilieren kann man Metriken angeben, die mit ausgeben werden sollen, z.B. die Genauigkeit (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy-Parameter übergeben \n",
    "model.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Daten ummappen, damit die Daten statt im Format \"28x28\" als 784 Zahlen representiert werden \n",
    "# 60000 Datensätze mit jeweils 784 Einträgen\n",
    "X_train_mapped = X_train.reshape(60000, 784)\n",
    "\n",
    "# Für die Ermittlung sollen nur T-Shirts (Kategorie 0) betrachtet werden. Deshalb wird das Array auf true/false (T-Shirt oder nicht) umgemappt\n",
    "y_train = y_train_categories == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genauigkeit wird beim Trainieren mit ausgegeben\n",
    "Nützlich, wenn das Trainieren länger läuft. Man hat so direktes Feedback, wie gut es bereits ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8464 - loss: 0.3407\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9264 - loss: 0.1888\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9427 - loss: 0.1628\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9492 - loss: 0.1497\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9505 - loss: 0.1423\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9522 - loss: 0.1353\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9522 - loss: 0.1312\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9535 - loss: 0.1259\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9529 - loss: 0.1258\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9547 - loss: 0.1216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2c8058bdd90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs: wie oft die Daten komplett durchlaufen werden\n",
    "# batch_size: schaue dir die ersten 1000 Bilder an und justiere anhand der Daten die Gewichte nach\n",
    "# weil immer nur sehr wenig justiert wird, muss man die Daten mehrfach durchlaufen\n",
    "model.fit(\n",
    "    X_train_mapped,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainingserfolg prüfen\n",
    "- Theoretisch kann man mit den vorhandenen Daten, mit denen man trainiert hat, testen.\n",
    "- Da hier aber etwas (z.B. ein Pixel an einer bestimmten Stelle) erkannt worden sein kann, sollte man sich Daten aufheben, mit denen man nicht lernt und diese später als Testlauf durchlaufen lassen -> Problem \"Overfitting\"\n",
    "- Gutes Maß: c.a. 75% Trainingsdaten / 25% Testdaten (bei neuronalen Netzen mit vielen Daten wird auch oft mit 10% Testdaten gearbeitet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 783us/step - accuracy: 0.9532 - loss: 0.1216\n",
      "[0.12187030166387558, 0.9535333514213562]\n",
      "['loss', 'compile_metrics']\n"
     ]
    }
   ],
   "source": [
    "# Test mit vorhandenen, trainierten Daten\n",
    "print(model.evaluate(X_train_mapped, y_train))\n",
    "# was sind die zwei ausgegebenen Zahlen?\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - accuracy: 0.9530 - loss: 0.1249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12708264589309692, 0.9524000287055969]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test mit Testdaten (-1: zähle selbst die Anzahl an Einträgen)\n",
    "X_test = open_images(\"data/fashion/t10k-images-idx3-ubyte.gz\").reshape(-1, 784)\n",
    "y_test = open_labels(\"data/fashion/t10k-labels-idx1-ubyte.gz\") == 0\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
